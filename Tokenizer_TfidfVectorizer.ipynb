{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "수강생선발과제_장수림.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "class Tokenizer():\n",
        "  \"\"\"단어 토큰화를 통해 어휘 사전을 구축하고 정수 인덱스값을 반환합니다.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  word_dict : dict\n",
        "      코퍼스에서 학습한 각각의 토큰과 정수 인덱스를 매핑한 어휘 사전입니다.\n",
        "  \n",
        "  fit_checker : bool\n",
        "      어휘 사전(`self.word_dict`)이 구축된 상태일 때 True를 반환합니다.\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> import Tokenizer\n",
        "  >>> sequences = [\n",
        "  ...              \"I like this movie, it's funny.\",\n",
        "  ...              \"I hate this movie.\", \n",
        "  ...              \"This was awesome! I like it.\",\n",
        "  ...              \"Nice one. I love it.\"\n",
        "                  ]\n",
        "  >>> tokenizer = Tokenizer()\n",
        "  >>> X = tokenizer.fit_transform(sequences)\n",
        "  >>> print(tokenizer.word_dict)\n",
        "  ...   {'oov': 0, 'i': 1, 'like': 2, 'this': 3, 'movie': 4, 'its': 5, 'funny': 6, 'hate': 7, 'was': 8, 'awesome': 9, 'it': 10, 'nice': 11, 'one': 12, 'love': 13}\n",
        "  >>> print([[round(num,2) for num in lst] for lst in X])\n",
        "  ...   [[1, 2, 3, 4, 5, 6], [1, 7, 3, 4], [3, 8, 9, 1, 2, 10], [11, 12, 1, 13, 10]]\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences): \n",
        "    \"\"\"텍스트 전처리를 하는 함수입니다.\n",
        "    \n",
        "    이 함수는 영어 문장으로 이루어진 코퍼스를 단어 토큰으로 변환합니다.\n",
        "    입력된 문장에 대해서 소문자로의 변환과 구두점 및 특수문자 제거를 수행합니다. \n",
        "    토큰화는 white space 단위로 수행합니다. (문자 공백, 탭, 줄바꿈, 리턴, 폼피드, 세로 탭을 포함)\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    이 함수는 의미있는 구두점과 특수문자는 제거하지 않고 유지하는 기능을 지원하지 않습니다.\n",
        "    사전 과제의 요구사항은 아니지만, 차후 보완 시에는 Penn Treebank Tokenization에 따라\n",
        "    (1)하이푼으로 구성된 단어는 하나로 유지하고 \n",
        "    (2)doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리하는 기능을 추가할 예정입니다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    result : nested list\n",
        "        각 문장을 토큰화한 결과로, nested list 형태입니다.\n",
        "    \"\"\"\n",
        "    punc = string.punctuation \n",
        "    result = [re.sub(r'[{}]'.format(punc),'',doc).lower().split() for doc in sequences]\n",
        "\n",
        "    return result \n",
        "     \n",
        "  def fit(self, sequences):\n",
        "    \"\"\"어휘 사전을 구축하는 함수입니다. \n",
        "\n",
        "    `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행한 뒤, \n",
        "    각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    이 함수는 `self.fit_checker=True`를 설정하는 것 외에 별도의 반환값이 없습니다. \n",
        "    \"\"\"\n",
        "    self.fit_checker = False\n",
        "    sequences = self.preprocessing(sequences)\n",
        "    idx = 1\n",
        "    for doc in sequences:\n",
        "      for token in doc:\n",
        "        if token not in list(self.word_dict.keys()):\n",
        "          self.word_dict.update({token:idx})\n",
        "          idx += 1\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    \"\"\"어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다. (`self.fit_checker=False`일 때만 실행)\n",
        "\n",
        "    `fit`(또는 `fit_transform`) 함수로 학습된 어휘 사전(`self.word_dict`)을 사용합니다.\n",
        "    어휘 사전에 없는 단어는 'oov'의 index로 변환합니다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    result : nested list\n",
        "        각 문장의 정수 인덱싱으로, nested list 형태입니다.\n",
        "    \"\"\"\n",
        "    sequences = self.preprocessing(sequences)\n",
        "    keys = self.word_dict.keys()\n",
        "    result = []\n",
        "    if self.fit_checker:\n",
        "      for doc in sequences:\n",
        "        idx = []\n",
        "        for token in doc:\n",
        "          if token in keys:\n",
        "            idx.append(self.word_dict.get(token))\n",
        "          else:\n",
        "            idx.append(self.word_dict.get('oov'))\n",
        "        result.append(idx) \n",
        "      return result\n",
        "    \n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    \"\"\"입력 문장을 정수 인덱싱하는 함수입니다.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    result : nested list\n",
        "        각 문장의 정수 인덱싱으로, nested list 형태입니다.\n",
        "    \"\"\"\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "N10nAUN3PxTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class TfidfVectorizer:\n",
        "  \"\"\"학습한 어휘 사전을 통해 TF-IDF를 계산해 리스트로 반환합니다.\n",
        "  \n",
        "  TF-IDF는 특정 단어가 문서 집합 내에서 얼마나 중요한 지를 나타내는 통계적 수치입니다.\n",
        "  TF(단어 빈도)와 IDF(역문서 빈도)의 곱으로 얻을 수 있습니다.\n",
        "    - TF(단어 빈도)는 학습한 문서에 각각의 단어가 나타난 횟수를 뜻합니다.\n",
        "    - IDF(역문서빈도)는 전체 문서의 수를 해당 단어를 포함한 문서의 수로 나눈 것을 의미합니다.\n",
        "\n",
        "  이에 관해 더 상세한 설명이 필요하다면 개별 함수의 해설란을 참조해주세요.\n",
        "  \n",
        "  Attributes\n",
        "  ----------\n",
        "  tokenizer : object\n",
        "      tokenizer를 불러옵니다.\n",
        "  fit_checker : bool\n",
        "      idf(`self.idf`)가 구해진 상태일 때 True를 반환합니다.\n",
        "  idf : list\n",
        "      idf 값을 저장한 list입니다.\n",
        "  tfidf_matrix : list\n",
        "      TF-IDF 행렬을 변환한 list입니다.\n",
        "\n",
        "  Examples\n",
        "  --------\n",
        "  >>> import Tokenizer\n",
        "  >>> import TfidfVectorizer\n",
        "  >>> sequences = [\n",
        "  ...              \"I like this movie, it's funny.\",\n",
        "  ...              \"I hate this movie.\", \n",
        "  ...              \"This was awesome! I like it.\",\n",
        "  ...              \"Nice one. I love it.\"\n",
        "                  ]\n",
        "  >>> tokenizer = Tokenizer()\n",
        "  >>> tfidfvectorizer = TfidfVectorizer(tokenizer)\n",
        "  >>> X = tfidfvectorizer.fit_transform(sequences)\n",
        "  >>> print(tfidfvectorizer.idf)\n",
        "  ...   [-0.2231435513142097, 0.28768207245178085, 0.0, ..., 0.6931471805599453, 0.6931471805599453, 0.6931471805599453]\n",
        "  >>> print(X)\n",
        "  ...   [[-0.22, 0.29, 0.0, ..., nan, nan, nan], ..., [-0.22, nan, nan, nan, ..., 0.69, 0.69, 0.69]]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "    self.idf = []\n",
        "    self.tfidf_matrix = []\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    \"\"\"입력 문장들을 이용해 IDF 값을 구하는 함수입니다.\n",
        "\n",
        "    IDF(역문서빈도)는 한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값입니다. \n",
        "    전체 문서의 수를 해당 단어를 포함한 문서의 수로 나눈 뒤 로그를 취하여 얻을 수 있습니다.\n",
        "\n",
        "    이 함수에서 IDF 값은 다음의 공식을 통해 계산됩니다.\n",
        "      idf(d,t) = log_e[ n / (1 + df(d,t)) ]\n",
        "      - df(d,t) : 단어 t가 포함된 문장 d의 개수\n",
        "      - n : 입력된 전체 문장 개수\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    이 함수는 `self.idf`를 저장하고 `self.fit_checker`를 활성화하는 것 외에 별도의 반환값이 없습니다. \n",
        "    \"\"\"\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)  \n",
        "    n = len(sequences)\n",
        "    df = {}\n",
        "    for d in tokenized:   \n",
        "      for t in set(d):    \n",
        "        if t in df.keys():\n",
        "          df[t] += 1\n",
        "        else:\n",
        "          df[t] = 1\n",
        "\n",
        "    self.idf = [np.log(n/(1+val)) for val in df.values()]\n",
        "\n",
        "    self.fit_checker = True\n",
        "\n",
        "  def transform(self, sequences):\n",
        "    \"\"\"입력 문장들을 이용해 TF-IDF 값을 구하는 함수입니다.\n",
        "\n",
        "    TF-IDF는 어떤 단어가 특정 문서 집합 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치입니다.\n",
        "    TF(단어 빈도)와 IDF(역문서 빈도)의 곱으로 얻을 수 있습니다.\n",
        "\n",
        "    이 함수에서 TF-IDF 값은 다음의 공식을 통해 계산됩니다.\n",
        "      tf-idf(d,t) = tf(d,t) * idf(d,t)\n",
        "      - tf(d, t) : 문장 d에 단어 t가 나타난 횟수\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    self.tfidf_matrix : list\n",
        "        Tf-idf 행렬을 변환한 list입니다. \n",
        "    \"\"\"\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      n_samples = len(sequences)\n",
        "      n_features = max([max(d) for d in tokenized])\n",
        "      self.tfidf_matrix = np.full((n_samples,n_features),np.nan)\n",
        "      # 단어 토큰이 해당 문장에 없을 경우 nan 값을 갖습니다.\n",
        "\n",
        "      tf = [[k.count(t) for t in k] for k in tokenized]\n",
        "\n",
        "      for idx_d, d in enumerate(tokenized):\n",
        "        for idx_t, t in enumerate(d):\n",
        "          self.tfidf_matrix[idx_d,t-1] = tf[idx_d][idx_t]*self.idf[t-1]\n",
        "\n",
        "      self.tfidf_matrix = self.tfidf_matrix.tolist()\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    \"\"\"입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences : list\n",
        "        여러 영어 문장이 포함된 list입니다.\n",
        "          \n",
        "    Returns\n",
        "    -------\n",
        "    self.tfidf_matrix : list\n",
        "        Tf-idf 행렬을 변환한 list입니다. \n",
        "    \"\"\"\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "EIwAJoDQP0xt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
